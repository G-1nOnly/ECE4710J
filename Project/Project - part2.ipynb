{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part II: Predicting Housing Prices - Build Your Own Model\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Scheme\n",
    "\n",
    "Your grade for the project will be based on your training RMSE and test RMSE. The thresholds are as follows:\n",
    "\n",
    "Points | 9 | 7 | 5 | 3\n",
    "--- | --- | --- | --- | ---\n",
    "Training RMSE | Less than 60k | [60k, 120k) | [120k, 200k) | More than 230k\n",
    "\n",
    "Points | 9 | 7 | 5 | 3\n",
    "--- | --- | --- | --- | ---\n",
    "Test RMSE | Less than 65k | [65k, 130k) | [130k, 230k) | More than 230k\n",
    "\n",
    "The top 20% of the submissions with the least testing errors will receive the additional two points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Imports You Might Need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Extract Dataset\n",
    "with zipfile.ZipFile('cook_county_contest_data.zip') as item:\n",
    "    item.extractall()\n",
    "    \n",
    "    \n",
    "### Note: we filtered the data in cook_county_contest_data, \n",
    "####so please use this dataset instead of the old one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "This notebook is specifically designed to guide you through the process of exporting your model's predictions on the test dataset for submission so you can see how your model performs.\n",
    "\n",
    "Most of what you have done in project part I should be transferrable here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Set up all the helper functions for your `process_data_fm` function.\n",
    "\n",
    "**Copy-paste all of the helper functions your `process_data_fm` need here in the following cell**. You **do not** have to fill out all of the functions in the cell below -- only fill out those that are actually useful to your feature engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_total_bedrooms(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): a data frame containing at least the Description column.\n",
    "    \"\"\"\n",
    "    with_rooms = data.copy()\n",
    "    with_rooms[\"Bedrooms\"] = with_rooms[\"Description\"].str.split(r\"(\\d+) of which are bedrooms\",expand=True)[1].astype(np.int64)\n",
    "    with_rooms[\"Bedrooms\"].fillna(0)\n",
    "    return with_rooms\n",
    "\n",
    "def add_total_rooms(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): a data frame containing at least the Description column.\n",
    "    \"\"\"\n",
    "    with_rooms = data.copy()\n",
    "    with_rooms[\"Rooms\"] = with_rooms[\"Description\"].str.split(r\"(It has a total of) (\\d+) (rooms)\",expand=True)[2].astype(np.int64)\n",
    "    with_rooms[\"Rooms\"].fillna(0)\n",
    "    return with_rooms\n",
    "\n",
    "def Log_all(data,*variables):\n",
    "    data_new = data.copy()\n",
    "    for x in variables:\n",
    "        x_new = 'Log '+x\n",
    "        data_new[x_new] = np.log(data_new[x])\n",
    "    return data_new   \n",
    "\n",
    "def replace_zero(data, *variables):\n",
    "    data_new = data.copy()\n",
    "    for x in variables:\n",
    "        data_new[x] = data_new.groupby('Neighborhood Code')[x].transform(lambda x: x.replace([0, np.nan, -np.inf], x.median()))\n",
    "    return data_new\n",
    "\n",
    "def replace_nuisance(data,*variables):\n",
    "    for x in variables:\n",
    "        idx = data[data[x]==-np.inf].index\n",
    "        data_new = data.drop(index = idx)\n",
    "        median = data_new[x].median()\n",
    "        data[x].replace(-np.inf,median,inplace=True)    \n",
    "    return data\n",
    "     \n",
    "def process_data_gm(data, pipeline_functions, prediction_col):\n",
    "    \"\"\"Process the data for a guided model.\"\"\"\n",
    "    for function, arguments, keyword_arguments in pipeline_functions:\n",
    "        if keyword_arguments and (not arguments):\n",
    "            data = data.pipe(function, **keyword_arguments)\n",
    "        elif (not keyword_arguments) and (arguments):\n",
    "            data = data.pipe(function, *arguments)\n",
    "        else:\n",
    "            data = data.pipe(function)\n",
    "    X = data.drop(columns=[prediction_col]).to_numpy()\n",
    "    y = data.loc[:, prediction_col].to_numpy()\n",
    "    return X, y\n",
    "\n",
    "def select_columns(data, *columns):\n",
    "    \"\"\"Select only columns passed as arguments.\"\"\"\n",
    "    return data.loc[:, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e123d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def ohe_roof_material(data):\n",
    "    \"\"\"\n",
    "    One-hot-encodes roof material.  New columns are of the form 0x_QUALITY.\n",
    "    \"\"\"\n",
    "    data['Roof Material'].replace({1:'Shingle/Asphalt',2:'Tar&Gravel',3:'Slate',4:'Shake',5:'Tile',6:'Other'},inplace=True)\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit([[\"Shingle/Asphalt\"],[\"Tar&Gravel\"],[\"Slate\"],[\"Shake\"],[\"Tile\"],[\"Other\"]])\n",
    "    ohe_cols = pd.DataFrame(enc.transform(data[['Roof Material']]).todense(), \n",
    "                           columns= enc.get_feature_names(),\n",
    "                           index = data.index)\n",
    "    ohe_cols = ohe_cols.rename(columns={'x0_Shingle/Asphalt':'rfm_Shingle/Asphalt','x0_Tar&Gravel':'rfm_Tar&Gravel','x0_Slate':'rfm_Slate',\n",
    "                             'x0_Shake':'rfm_Shake','x0_Tile':'rfm_Tile','x0_Other':'rfm_Other'})\n",
    "    data = data.join([ohe_cols])\n",
    "    return data\n",
    "\n",
    "def find_expensive_neighborhoods(data, n=3, metric=np.median):\n",
    "    neighborhoods_price = data.groupby('Log Sale Price').agg(metric).sort_values(by='Log Sale Price',ascending=False).head(n).index\n",
    "    neighborhoods = data.loc[data['Log Sale Price'].isin(neighborhoods_price)]['Neighborhood Code'].values\n",
    "    return [int(code) for code in neighborhoods]\n",
    "\n",
    "def add_in_expensive_neighborhood(data, neighborhoods):\n",
    "    data['in_expensive_neighborhood'] = data['Neighborhood Code'].isin(neighborhoods).astype('int')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dffed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_test = pd.read_csv('cook_county_contest_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the covariance to see the importance of the feature\n",
    "Feature_test = add_total_bedrooms(Feature_test)\n",
    "Feature_test = add_total_rooms(Feature_test)\n",
    "Feature_test = ohe_roof_material(Feature_test)\n",
    "features = ['Bedrooms','Rooms','Latitude','Estimate (Building)','Estimate (Land)','Property Class','Building Square Feet']\n",
    "Feature_test = Log_all(Feature_test,*features)\n",
    "Feature_test['Log Sale Price'] = np.log(Feature_test['Sale Price'])\n",
    "expensive_neighborhoods = find_expensive_neighborhoods(Feature_test, 20, np.median)\n",
    "Feature_test = add_in_expensive_neighborhood(Feature_test, expensive_neighborhoods)\n",
    "Feature_test = Feature_test.drop(columns=[\"Roof Material\",\"Description\",\"Pure Market Filter\",\"Use\"])\n",
    "\n",
    "Goal = Feature_test[['Sale Price']]\n",
    "Feature_test = Feature_test.drop(columns=[\"Sale Price\"])\n",
    "def heatmap(index):\n",
    "    df_temp = Feature_test.iloc[:,10*index:10*(index+1)]\n",
    "    df_temp[\"Sale Price\"] = Goal\n",
    "    sns.heatmap(df_temp.corr(),vmin=0, vmax=1,cmap=\"YlGnBu\",annot=True,fmt='.2f')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{index}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = Feature_test[[\"Log Estimate (Building)\",\"Log Estimate (Land)\"]]\n",
    "temp[\"Sale Price\"] = Goal\n",
    "sns.heatmap(temp.corr(),vmin=0, vmax=1,cmap=\"YlGnBu\",annot=True,fmt='.2f')  \n",
    "plt.show()                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_selected = Feature_test[['Log Sale Price','Log Bedrooms','Log Rooms','Log Latitude','Log Estimate (Building)','Log Estimate (Land)',\n",
    "                     'Fireplaces','Log Property Class','Log Building Square Feet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_selected.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3496b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "temp = add_total_bedrooms(temp)\n",
    "temp = add_total_rooms(temp)\n",
    "temp = ohe_roof_material(temp)\n",
    "temp = temp.drop(columns=[\"Roof Material\",\"Description\",\"Pure Market Filter\",\"Use\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Property Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_fireplaces(data):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit([[0],[1],[2]])\n",
    "    ohe_cols = pd.DataFrame(enc.transform(data[['Fireplaces']]).todense(), \n",
    "                           columns= enc.get_feature_names(),\n",
    "                           index = data.index)\n",
    "    ohe_cols = ohe_cols.rename(columns={'x0_0':'Fireplaces0','x0_1':'Fireplaces1','x0_2':'Fireplaces2'})\n",
    "    data = data.join([ohe_cols])\n",
    "    return data\n",
    "\n",
    "def ohe_propertyclass(data):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit([[202],[203],[204],[205],[206],[207],[208],[209],[278]])\n",
    "    ohe_cols = pd.DataFrame(enc.transform(data[['Property Class']]).todense(), \n",
    "                           columns= enc.get_feature_names(),\n",
    "                           index = data.index)\n",
    "    ohe_cols = ohe_cols.rename(columns={'x0_202':'Propertyclass0','x0_203':'Propertyclass1','x0_204':'Propertyclass2',\n",
    "                                        'x0_205':'Propertyclass3','x0_206':'Propertyclass4','x0_207':'Propertyclass5',\n",
    "                                        'x0_208':'Propertyclass6','x0_209':'Propertyclass7','x0_278':'Propertyclass8'})\n",
    "    data = data.join([ohe_cols])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "temp = ohe_fireplaces(temp)\n",
    "temp = ohe_propertyclass(temp)\n",
    "temp['Log Sale Price'] = np.log(temp['Sale Price'])\n",
    "temp_f = temp[['Log Sale Price','Fireplaces0','Fireplaces1','Fireplaces2']]\n",
    "sns.heatmap(temp_f.corr(),vmin=0, vmax=1,cmap=\"YlGnBu\",annot=True,fmt='.2f')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76404aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_p = temp[['Log Sale Price','Propertyclass0','Propertyclass1','Propertyclass2',\n",
    "               'Propertyclass3','Propertyclass4','Propertyclass5','Propertyclass6',\n",
    "               'Propertyclass7','Propertyclass8']]\n",
    "sns.heatmap(temp_p.corr(),vmin=0, vmax=1,cmap=\"YlGnBu\",annot=True,fmt='.2f')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cc036",
   "metadata": {},
   "source": [
    "Final Choice (|Cov|>0.3)\n",
    "\n",
    "1.Log Building Square Feet\n",
    "2.Rooms\n",
    "3.Bedrooms\n",
    "4.Latitude\n",
    "5.Log Estimate Building\n",
    "6.Log Estimate Land\n",
    "7.Fireplaces\n",
    "8.Property Class\n",
    "9.Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Setup your `process_data_fm` function\n",
    "\n",
    "**Create your implementation of `process_data_fm` from into the following cell.**\n",
    "\n",
    "Here are a few additional things **you should check and change to make sure your `process_data_fm` function satisfies**:\n",
    "- Unlike part 1, we will not be expecting your `process_data_fm` function to return both the design matrix `X` and the observed target vector `y`; your function should now **only return X**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please include all of your feature engineering process inside this function.\n",
    "# Do not modify the parameters of the function below. \n",
    "# Note that data will no longer have the column Sale Price in it directly, so plan your feature engineering process around that.\n",
    "def process_data_fm(data):\n",
    "    # Replace the following line with your own feature engineering pipeline\n",
    "    X = data.copy()\n",
    "    \n",
    "    pipeline = [\n",
    "    (add_total_bedrooms,False,False),\n",
    "    (add_total_rooms,False,False),\n",
    "    # (ohe_propertyclass,False,False),\n",
    "    # (ohe_fireplaces,False,False),\n",
    "    (replace_zero,['Bedrooms','Rooms','Latitude','Estimate (Building)','Estimate (Land)',\n",
    "                     'Property Class','Building Square Feet'],False),\n",
    "    (Log_all,['Bedrooms','Rooms','Latitude','Estimate (Building)','Estimate (Land)','Building Square Feet','Property Class'],False),\n",
    "    (select_columns,['Log Bedrooms','Log Rooms','Log Latitude','Log Estimate (Building)','Log Estimate (Land)',\n",
    "                     'Log Building Square Feet','Fireplaces','Log Property Class'\n",
    "                    #  'Fireplaces0','Fireplaces1',\n",
    "                    # #  'Fireplaces2',\n",
    "                    #  'Propertyclass0','Propertyclass8'\n",
    "                    #  'Propertyclass1','Propertyclass2','Propertyclass3','Propertyclass4',\n",
    "                    #  'Propertyclass5','Propertyclass6','Propertyclass7'\n",
    "                     ],False),\n",
    "    ]\n",
    "    \n",
    "    for function, arguments, keyword_arguments in pipeline:\n",
    "        if keyword_arguments and (not arguments):\n",
    "            X = X.pipe(function, **keyword_arguments)\n",
    "        elif (not keyword_arguments) and (arguments):\n",
    "            X = X.pipe(function, *arguments)\n",
    "        else:\n",
    "            X = X.pipe(function)\n",
    "            \n",
    "    X = X.to_numpy()\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03510170",
   "metadata": {},
   "source": [
    "## Step 3. Train your model\n",
    "\n",
    "Run the following cell to import the new set of training data to fit your model on. **You can use any regression model, the following is just an example** If your `process_data_fm` satisfies all the specified requirements, the cell should run without any error.\n",
    "\n",
    "**As usual**, your model will predict the log-transformed sale price, and our grading will transform your predictions back to the normal vlaues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multiple features linear regression\n",
    "train_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "y_train = np.log(train_data['Sale Price'])\n",
    "train_data = train_data.drop(columns=['Sale Price'])\n",
    "X_train = process_data_fm(train_data)\n",
    "model = lm.LinearRegression(fit_intercept=True)\n",
    "###You can use other models\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    return np.sqrt(np.mean((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17333bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(data,percentage=0.8):\n",
    "    data_len = data.shape[0]\n",
    "    shuffled_indices = np.random.permutation(data_len)\n",
    "    split_point = int(data_len * percentage)\n",
    "    train_idx = shuffled_indices[:split_point]\n",
    "    test_idx = shuffled_indices[split_point:]\n",
    "    return train_idx,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdca36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "# Including Engineering Feature, Lasso, Ridge and Elasticnet Model\n",
    "def Train_Test_RMSE(seed_num,regs):\n",
    "    np.random.seed(seed_num)\n",
    "    full_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "    train_set = full_data.iloc[train_test_split(full_data)[0]]\n",
    "    test_set = full_data.iloc[train_test_split(full_data)[1]]\n",
    "\n",
    "    y_tr = np.log(train_set['Sale Price'])\n",
    "    train_set = train_set.drop(columns=['Sale Price'])\n",
    "    x_tr = process_data_fm(train_set)\n",
    "    y_te = np.log(test_set['Sale Price'])\n",
    "    test_set = test_set.drop(columns=['Sale Price'])\n",
    "    x_te = process_data_fm(test_set)\n",
    "    \n",
    "    lm1 = lm.LinearRegression(fit_intercept=True)\n",
    "    lm1.fit(x_tr, y_tr);\n",
    "    lm2 = lm.RidgeCV(alphas=regs,fit_intercept=True)\n",
    "    lm2.fit(x_tr, y_tr);\n",
    "    lm3 = lm.LassoCV(alphas=regs,fit_intercept=True)\n",
    "    lm3.fit(x_tr, y_tr);\n",
    "    lm4 = lm.ElasticNetCV(alphas=regs,fit_intercept=True)\n",
    "    lm4.fit(x_tr, y_tr);\n",
    "\n",
    "    y_tr_predicted1 = lm1.predict(x_tr)\n",
    "    y_te_predicted1 = lm1.predict(x_te)\n",
    "    y_tr_predicted2 = lm2.predict(x_tr)\n",
    "    y_te_predicted2 = lm2.predict(x_te)\n",
    "    y_tr_predicted3 = lm3.predict(x_tr)\n",
    "    y_te_predicted3 = lm3.predict(x_te)\n",
    "    y_tr_predicted4 = lm4.predict(x_tr)\n",
    "    y_te_predicted4 = lm4.predict(x_te)\n",
    "\n",
    "    train_rmse1 = rmse(np.exp(y_tr_predicted1),np.exp(y_tr))\n",
    "    test_rmse1 = rmse(np.exp(y_te_predicted1),np.exp(y_te))\n",
    "    train_rmse2 = rmse(np.exp(y_tr_predicted2),np.exp(y_tr))\n",
    "    test_rmse2 = rmse(np.exp(y_te_predicted2),np.exp(y_te))\n",
    "    train_rmse3 = rmse(np.exp(y_tr_predicted3),np.exp(y_tr))\n",
    "    test_rmse3 = rmse(np.exp(y_te_predicted3),np.exp(y_te))\n",
    "    train_rmse4 = rmse(np.exp(y_tr_predicted4),np.exp(y_tr))\n",
    "    test_rmse4 = rmse(np.exp(y_te_predicted4),np.exp(y_te))\n",
    "    \n",
    "    print(\"Using seed_number: \",seed_num)\n",
    "    print(\"Training RMSE: \",train_rmse1)\n",
    "    print(\"Testing RMSE: \",test_rmse1)\n",
    "    print(\"---------------------\")\n",
    "    print(\"Ridge Training RMSE: \",train_rmse2)\n",
    "    print(\"Ridge Testing RMSE: \",test_rmse2)\n",
    "    print(\"---------------------\")\n",
    "    print(\"Lasso Training RMSE: \",train_rmse3)\n",
    "    print(\"Lasso Testing RMSE: \",test_rmse3)\n",
    "    print(\"---------------------\")\n",
    "    print(\"Elasticnet Training RMSE: \",train_rmse4)\n",
    "    print(\"Elasticnet Testing RMSE: \",test_rmse4)\n",
    "    print(\"---------------------\")\n",
    "    \n",
    "    return lm1,lm2,lm3,lm4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_alpha = []\n",
    "for i in range(1,9):\n",
    "    ls_alpha.extend([0.0001*i,0.001*i,0.01*i,0.1*i])\n",
    "ls_alpha.append(1)\n",
    " \n",
    "seed_num = np.random.randint(1000,2000)\n",
    "\n",
    "print(\"Iteration begin\")\n",
    "lm1,lm2,lm3,lm4 = Train_Test_RMSE(seed_num,ls_alpha)\n",
    "print(\"Iteration end\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xgboost model\n",
    "import xgboost as xgb\n",
    "\n",
    "rand_xgb_seed = np.random.randint(1000,2000)\n",
    "np.random.seed(rand_xgb_seed)\n",
    "\n",
    "def eval_train_test_split(tr_size=0.8,val_size=0.1):\n",
    "    full_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "\n",
    "    train_set = full_data.iloc[train_test_split(full_data,tr_size)[0]]\n",
    "    test_set = full_data.iloc[train_test_split(full_data,tr_size)[1]]\n",
    "\n",
    "    #Train and test overall Treat the test part as unknown for prediction\n",
    "    train_set_all = train_set.copy()\n",
    "    y_tr_xgb_all = np.log(train_set_all['Sale Price'])\n",
    "    train_set_all = train_set_all.drop(columns=['Sale Price'])\n",
    "    x_tr_xgb_all = process_data_fm(train_set_all)\n",
    "    y_te_xgb = np.log(test_set['Sale Price'])\n",
    "    test_set = test_set.drop(columns=['Sale Price'])\n",
    "    x_te_xgb = process_data_fm(test_set)\n",
    "\n",
    "    #Train_test_split the training set for xgboost validation\n",
    "    train_set_tr = train_set.iloc[train_test_split(train_set,1-val_size)[0]]\n",
    "    train_set_val = train_set.iloc[train_test_split(train_set,1-val_size)[1]]\n",
    "\n",
    "    y_tr_xgb = np.log(train_set_tr['Sale Price'])\n",
    "    train_set_tr = train_set_tr.drop(columns=['Sale Price'])\n",
    "    x_tr_xgb = process_data_fm(train_set_tr)\n",
    "    y_val = np.log(train_set_val['Sale Price'])\n",
    "    train_set_val = train_set_val.drop(columns=['Sale Price'])\n",
    "    x_val = process_data_fm(train_set_val)\n",
    "    return x_tr_xgb,y_tr_xgb,x_te_xgb,y_te_xgb,x_val,y_val,x_tr_xgb_all,y_tr_xgb_all\n",
    "    \n",
    "\n",
    "def xgboost_test():\n",
    "    x_tr,y_tr,x_te,y_te = train_test_set()\n",
    "    ls = [0.25,0.26,0.27,0.28,0.29,0.3]\n",
    "    test_score = []\n",
    "    for x in ls:\n",
    "        xgb_model = xgb.XGBRegressor(objective='reg:squarederror',booster='dart',max_depth=8,n_estimators=300,learning_rate=x)\n",
    "        test_score.append(np.mean(np.sqrt(-cross_val_score(xgb_t,x_tr,y_tr,cv=5,scoring='neg_mean_squared_error'))))\n",
    "        xgb_model.fit(x_tr,y_tr,verbose=True)\n",
    "        train_xgb = xgb_model.predict(x_tr)\n",
    "        test_xgb = xgb_model.predict(x_te)\n",
    "        train_rmse = rmse(np.exp(train_xgb),np.exp(y_tr))\n",
    "        test_rmse = rmse(np.exp(test_xgb),np.exp(y_te))\n",
    "        print(\"Training RMSE: \",train_rmse)\n",
    "        print(\"Testing RMSE: \",test_rmse)\n",
    "        print(f\"Finished testing {x}\")\n",
    "    plt.plot(ls,test_score)\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "def xgboost():\n",
    "    x_tr,y_tr,x_te,y_te = train_test_set()\n",
    "    xgb_model = xgb.XGBRegressor(learing_rate=0.3,objective='reg:squarederror',booster='dart',max_depth=6,reg_lambda=3,n_estimators=300)\n",
    "    xgb_model.fit(x_tr,y_tr,verbose=True)\n",
    "    train_xgb = xgb_model.predict(x_tr)\n",
    "    test_xgb = xgb_model.predict(x_te)\n",
    "    train_rmse = rmse(np.exp(train_xgb),np.exp(y_tr))\n",
    "    test_rmse = rmse(np.exp(test_xgb),np.exp(y_te))\n",
    "    print(\"Training RMSE: \",train_rmse)\n",
    "    print(\"Testing RMSE: \",test_rmse)\n",
    "    return xgb_model\n",
    "    \n",
    "\n",
    "def xgboost_eval():\n",
    "    x_tr_xgb,y_tr_xgb,x_te_xgb,y_te_xgb,x_val,y_val,x_tr_xgb_all,y_tr_xgb_all = eval_train_test_split(0.8,0.1)\n",
    "    xgb_model = xgb.XGBRegressor(learing_rate=0.3,objective='reg:squarederror',booster='dart',max_depth=10,reg_lambda=5,n_estimators=300)\n",
    "    xgb_model.fit(x_tr_xgb, y_tr_xgb,eval_metric='rmse',verbose = True, eval_set = [(x_val,y_val)],early_stopping_rounds=50)\n",
    "    train_rmse_xgb = rmse(np.exp(xgb_model.predict(x_tr_xgb_all)),np.exp(y_tr_xgb_all))\n",
    "    test_rmse_xgb = rmse(np.exp(xgb_model.predict(x_te_xgb)),np.exp(y_te_xgb))\n",
    "    print(\"Xgboost Training RMSE: \",train_rmse_xgb)\n",
    "    print(\"Xgboost Testing RMSE: \",test_rmse_xgb)\n",
    "    return xgb_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ffebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5718afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_RMSE(model):\n",
    "    train_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "    train_y = np.log(train_data['Sale Price'])\n",
    "    train_data = train_data.drop(columns=['Sale Price'])\n",
    "    train_x  = process_data_fm(train_data)\n",
    "    all_rmse = rmse(np.exp(model.predict(train_x)),np.exp(train_y))\n",
    "    print(\"Overall RMSE: \", all_rmse)\n",
    "    return all_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgboost_eval()\n",
    "overall_RMSE(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set preparation\n",
    "def train_test_set():\n",
    "    full_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "    train_set = full_data.iloc[train_test_split(full_data)[0]]\n",
    "    test_set = full_data.iloc[train_test_split(full_data)[1]]\n",
    "    y_tr = np.log(train_set['Sale Price'])\n",
    "    train_set = train_set.drop(columns=['Sale Price'])\n",
    "    x_tr = process_data_fm(train_set)\n",
    "    y_te = np.log(test_set['Sale Price'])\n",
    "    test_set = test_set.drop(columns=['Sale Price'])\n",
    "    x_te = process_data_fm(test_set)\n",
    "    return x_tr,y_tr,x_te,y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba689cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Regressor model\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def GBR_Test():\n",
    "    x_tr,y_tr,x_te,y_te = train_test_set()\n",
    "    ls = [0.01,0.02,0.03,0.04,0.05,0.1]\n",
    "    test_score = []\n",
    "    for x in ls:\n",
    "        GBR = GradientBoostingRegressor(n_estimators=3600, learning_rate=x,loss='huber',warm_start=True) \n",
    "        test_score.append(np.mean(np.sqrt(-cross_val_score(GBR,x_tr,y_tr,cv=5,scoring='neg_mean_squared_error'))))\n",
    "        print(f\"Finished testing {x}\")\n",
    "    plt.plot(ls,test_score)\n",
    "    plt.show()   \n",
    "\n",
    "def GBR():\n",
    "    x_tr,y_tr,x_te,y_te = train_test_set()\n",
    "    GBR = GradientBoostingRegressor(n_estimators=3600, learning_rate=0.03,loss='huber', random_state = 5,max_depth=10,\n",
    "                                    tol=1e-3,warm_start=True,validation_fraction=0.2) \n",
    "    GBR.fit(x_tr,y_tr)\n",
    "    train_GBR = GBR.predict(x_tr)\n",
    "    test_GBR = GBR.predict(x_te)\n",
    "    train_rmse = rmse(np.exp(train_GBR),np.exp(y_tr))\n",
    "    test_rmse = rmse(np.exp(test_GBR),np.exp(y_te))\n",
    "    print(\"Training RMSE: \",train_rmse)\n",
    "    print(\"Testing RMSE: \",test_rmse)\n",
    "    return GBR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a037f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e698295",
   "metadata": {},
   "outputs": [],
   "source": [
    "myGBR = GBR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7127855",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_RMSE(myGBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_all():\n",
    "    full_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "    GBR = GradientBoostingRegressor(n_estimators=3600, learning_rate=0.02,loss='huber', random_state = 5,max_depth=10,\n",
    "                                    tol=1e-3,warm_start=True,validation_fraction=0.2) \n",
    "    y = np.log(full_data['Sale Price'])\n",
    "    full_data = full_data.drop(columns=['Sale Price'])\n",
    "    x = process_data_fm(full_data)\n",
    "    GBR.fit(x,y)\n",
    "    \n",
    "    train_GBR = GBR.predict(x)\n",
    "    rmse_all = rmse(np.exp(train_GBR),np.exp(y))\n",
    "    print(\"RMSE: \",rmse_all)\n",
    "    return GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07040fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "       \n",
    "def RandomForest(max_f,n_es=100,rand_s=5):\n",
    "    x_tr,y_tr,x_te,y_te = train_test_set()\n",
    "    RFR = RandomForestRegressor(n_estimators=n_es,random_state=rand_s,max_features=max_f,min_samples_leaf=1,max_depth=80)\n",
    "    RFR.fit(x_tr,y_tr)\n",
    "    train_RFR = RFR.predict(x_tr)\n",
    "    test_RFR = RFR.predict(x_te)\n",
    "    train_rmse = rmse(np.exp(train_RFR),np.exp(y_tr))\n",
    "    test_rmse = rmse(np.exp(test_RFR),np.exp(y_te))\n",
    "    print(\"Training RMSE: \",train_rmse)\n",
    "    print(\"Testing RMSE: \",test_rmse)\n",
    "    return RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_all():\n",
    "    full_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "    RFR = RandomForestRegressor(n_estimators=75,random_state=10,max_features=0.25,min_samples_leaf=1,max_depth=80)\n",
    "    y = np.log(full_data['Sale Price'])\n",
    "    full_data = full_data.drop(columns=['Sale Price'])\n",
    "    x = process_data_fm(full_data)\n",
    "    RFR.fit(x,y)\n",
    "    \n",
    "    train_RFR = RFR.predict(x)\n",
    "    rmse_all = rmse(np.exp(train_RFR),np.exp(y))\n",
    "    print(\"RMSE: \",rmse_all)\n",
    "    return RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRFR = RandomForest(0.25,n_es=75,rand_s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_RMSE(myRFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b61a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaboost\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def Adaboost_test():\n",
    "    x_tr,y_tr,x_te,y_te = train_test_set()\n",
    "    ls = [0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5]\n",
    "    test_score = []\n",
    "    for x in ls:\n",
    "        ada = AdaBoostRegressor(DecisionTreeRegressor(max_features=0.4,max_depth=20),random_state=10,learning_rate=x,n_estimators=175)\n",
    "        test_score.append(np.mean(np.sqrt(-cross_val_score(ada,x_tr,y_tr,cv=5,scoring='neg_mean_squared_error'))))\n",
    "        print(f\"Finished testing {x}\")\n",
    "    plt.plot(ls,test_score)\n",
    "    plt.show()   \n",
    "        \n",
    "        \n",
    "def Adaboost(rand_s=10):\n",
    "    x_tr,y_tr,x_te,y_te = train_test_set()\n",
    "    ada = AdaBoostRegressor(DecisionTreeRegressor(max_features=0.4,max_depth=20),n_estimators=175,random_state=rand_s,learning_rate=0.05)\n",
    "    ada.fit(x_tr,y_tr)\n",
    "    train = ada.predict(x_tr)\n",
    "    test= ada.predict(x_te)\n",
    "    train_rmse = rmse(np.exp(train),np.exp(y_tr))\n",
    "    test_rmse = rmse(np.exp(test),np.exp(y_te))\n",
    "    print(\"Training RMSE: \",train_rmse)\n",
    "    print(\"Testing RMSE: \",test_rmse)\n",
    "    return ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98180c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adaboost_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myada = Adaboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34029d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_RMSE(myada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d553e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_all():\n",
    "    full_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "    ada = AdaBoostRegressor(DecisionTreeRegressor(max_features=0.4,max_depth=20),n_estimators=175,random_state=10,learning_rate=0.05)\n",
    "    y = np.log(full_data['Sale Price'])\n",
    "    full_data = full_data.drop(columns=['Sale Price'])\n",
    "    x = process_data_fm(full_data)\n",
    "    ada.fit(x,y)\n",
    "    \n",
    "    train_ada = ada.predict(x)\n",
    "    rmse_all = rmse(np.exp(train_ada),np.exp(y))\n",
    "    print(\"RMSE: \",rmse_all)\n",
    "    return ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c208323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split as auto_train_test_split\n",
    "\n",
    "def prepare_stacking(model, x_train, y_train, x_test, n_folds=3):\n",
    "    \n",
    "    train_num, test_num = x_train.shape[0], x_test.shape[0]\n",
    "    second_level_train_set = np.zeros((train_num,))\n",
    "    second_level_test_set = np.zeros((test_num,))\n",
    "    test_nfolds_sets = np.zeros((test_num, n_folds))\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "\n",
    "    for i,(train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tr, y_tr = x_train[train_index], y_train[train_index]\n",
    "        x_te, y_te =  x_train[test_index], y_train[test_index]\n",
    "        model.fit(x_tr, y_tr)\n",
    "        second_level_train_set[test_index] = model.predict(x_te)\n",
    "        test_nfolds_sets[:,i] = model.predict(x_test)\n",
    "        \n",
    "    second_level_test_set[:] = test_nfolds_sets.mean(axis=1)\n",
    "    return second_level_train_set, second_level_test_set\n",
    "\n",
    "def Stacking(level1_models,level2_model,nfolds=3):  \n",
    "    train_data = pd.read_csv('cook_county_contest_train.csv', index_col='Unnamed: 0')\n",
    "    train_y = np.log(train_data['Sale Price']).to_numpy()\n",
    "    train_data = train_data.drop(columns=['Sale Price'])\n",
    "    train_x  = process_data_fm(train_data)\n",
    "    x_tr,x_te,y_tr,y_te = auto_train_test_split(train_x,train_y,test_size=0.2)\n",
    "    \n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    i = 1\n",
    "    for m in level1_models:\n",
    "        x_tr_level2,x_te_level2 = prepare_stacking(m,x_tr,y_tr,x_te,n_folds=nfolds)\n",
    "        train_set.append(x_tr_level2)\n",
    "        test_set.append(x_te_level2)\n",
    "        print(f\"Model {i} Train finished!\")\n",
    "        i = i+1\n",
    "    \n",
    "    level2_train = np.concatenate([res.reshape(-1,1) for res in train_set], axis=1)\n",
    "    level2_test = np.concatenate([res.reshape(-1,1) for res in test_set], axis=1)    \n",
    "    level2_model.fit(level2_train, y_tr);\n",
    "    \n",
    "    train_rmse = rmse(np.exp(level2_model.predict(level2_train)),np.exp(y_tr))\n",
    "    test_rmse= rmse(np.exp(level2_model.predict(level2_test)),np.exp(y_te))\n",
    "    print(\"Training RMSE: \",train_rmse)\n",
    "    print(\"Testing RMSE: \",test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff573a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ada_s = AdaBoostRegressor(DecisionTreeRegressor(max_features=0.4,max_depth=20),n_estimators=175,random_state=10,learning_rate=0.05)\n",
    "RFR_s = RandomForestRegressor(n_estimators=75,random_state=10,max_features=0.25,min_samples_leaf=1,max_depth=80)\n",
    "GBR_s = GradientBoostingRegressor(n_estimators=3600, learning_rate=0.03,loss='huber', random_state = 5,max_depth=10,\n",
    "                                    tol=1e-3,warm_start=True,validation_fraction=0.2) \n",
    "xgb_s = xgb.XGBRegressor(learing_rate=0.3,objective='reg:squarederror',booster='dart',max_depth=6,reg_lambda=3,n_estimators=100)\n",
    "\n",
    "Ada_t = AdaBoostRegressor(DecisionTreeRegressor(max_features=0.4))\n",
    "xgb_t = xgb.XGBRegressor(objective='reg:squarederror',booster='dart')\n",
    "GBR_t = GradientBoostingRegressor(learning_rate=0.03,loss='huber',tol=1e-3) \n",
    "RFR_t = RandomForestRegressor(max_features=0.25,n_estimators=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af65f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking([GBR_s,RFR_s,Ada_s,xgb_s],GBR_t,nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking([GBR_s,RFR_s,Ada_s,xgb_s],xgb_t,nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5302028",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking([GBR_s,RFR_s,Ada_s,xgb_s],xgb_s,nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking([GBR_s,xgb_s],lm.Ridge(fit_intercept=True),nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017dcbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking([GBR_s,xgb_s,Ada_s],lm.Ridge(fit_intercept=True),nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a84b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking([GBR_s,Ada_s,RFR_s],lm.Ridge(fit_intercept=True),nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_model = ['Mylm','Ridge','Lasso','Elasticnet','xgboost','GradientBoosting','RandomForest','Adaboost']\n",
    "ls_rmse_m = [lm1,lm2,lm3,lm4,xgb_model,myGBR,myRFR,myada]\n",
    "ls_rmse = []\n",
    "for x in ls_rmse_m:\n",
    "    ls_rmse.append(overall_RMSE(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.bar(x=ls_model, height=ls_rmse)\n",
    "ax.set_title(\"Model Overall RMSE\", fontsize=20)\n",
    "for a, b in zip(ls_model, ls_rmse):\n",
    "    plt.text(a, b + 0.05, '%.0f' % b, ha='center', va='bottom', fontsize=12,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd65cb6",
   "metadata": {},
   "source": [
    "Based on the above observation, we choose Random_Forest as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9a411",
   "metadata": {},
   "source": [
    "## Step 4. Make Predictions on the Test Dataset\n",
    "\n",
    "Run the following cell to estimate the sale price on the test dataset and export your model's predictions as a csv file called `predictions.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c964013",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('cook_county_contest_test.csv', index_col='Unnamed: 0')\n",
    "X_test = process_data_fm(test_data)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "y_test_predicted = np.exp(y_test_predicted)\n",
    "predictions = pd.DataFrame({'Sale Price': y_test_predicted})\n",
    "predictions.to_csv('predictions.csv')\n",
    "print('Your predictions have been exported as predictions.csv. Please download the file and submit it to Canvas. ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
